digraph {
	graph [size="67.8,67.8"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2564833755456 [label="
 (1, 1, 32, 32)" fillcolor=darkolivegreen1]
	2564833759296 [label=_uqBackward]
	2564833740256 -> 2564833759296
	2564833740256 [label=TanhBackward]
	2564833739920 -> 2564833740256
	2564833739920 [label=SlowConvTranspose2DBackward]
	2564833689376 -> 2564833739920
	2564833689376 [label=_uqBackward]
	2564833740496 -> 2564833689376
	2564833740496 [label=ReluBackward1]
	2564833740640 -> 2564833740496
	2564833740640 [label=NativeBatchNormBackward]
	2564833740736 -> 2564833740640
	2564833740736 [label=SlowConvTranspose2DBackward]
	2564833688928 -> 2564833740736
	2564833688928 [label=_uqBackward]
	2564833741024 -> 2564833688928
	2564833741024 [label=ReluBackward0]
	2564833741168 -> 2564833741024
	2564833741168 [label=NativeBatchNormBackward]
	2564833741264 -> 2564833741168
	2564833741264 [label=SlowConvTranspose2DBackward]
	2564833688480 -> 2564833741264
	2564833688480 [label=_uqBackward]
	2564833741552 -> 2564833688480
	2564833741552 [label=ReluBackward0]
	2564833741696 -> 2564833741552
	2564833741696 [label=NativeBatchNormBackward]
	2564833741792 -> 2564833741696
	2564833741792 [label=SlowConvTranspose2DBackward]
	2564831059520 -> 2564833741792
	2564831059520 [label=_uqBackward]
	2564833742080 -> 2564831059520
	2564833742080 [label=ReluBackward0]
	2564833742224 -> 2564833742080
	2564833742224 [label=NativeBatchNormBackward]
	2564833742320 -> 2564833742224
	2564833742320 [label=SlowConvTranspose2DBackward]
	2564833742512 -> 2564833742320
	2564833742512 [label=ViewBackward]
	2564833742656 -> 2564833742512
	2564833742656 [label=AddmmBackward]
	2564833742752 -> 2564833742656
	2564830903424 [label="decoder_input.block.bias
 (1024)" fillcolor=lightblue]
	2564830903424 -> 2564833742752
	2564833742752 [label=AccumulateGrad]
	2564833742704 -> 2564833742656
	2564833742704 [label=AddBackward0]
	2564833783872 -> 2564833742704
	2564833783872 [label=MulBackward0]
	2564833784112 -> 2564833783872
	2564833784112 [label=ExpBackward]
	2564833784208 -> 2564833784112
	2564833784208 [label=MulBackward0]
	2564833784304 -> 2564833784208
	2564833784304 [label=AddmmBackward]
	2564833784400 -> 2564833784304
	2564830903360 [label="fc_var.block.bias
 (128)" fillcolor=lightblue]
	2564830903360 -> 2564833784400
	2564833784400 [label=AccumulateGrad]
	2564833784352 -> 2564833784304
	2564833784352 [label=ViewBackward]
	2564831058624 -> 2564833784352
	2564831058624 [label=_uqBackward]
	2564833784640 -> 2564831058624
	2564833784640 [label=ReluBackward0]
	2564833784784 -> 2564833784640
	2564833784784 [label=NativeBatchNormBackward]
	2564833784880 -> 2564833784784
	2564833784880 [label=ThnnConv2DBackward]
	2564831059072 -> 2564833784880
	2564831059072 [label=_uqBackward]
	2564833785168 -> 2564831059072
	2564833785168 [label=ReluBackward0]
	2564833785312 -> 2564833785168
	2564833785312 [label=NativeBatchNormBackward]
	2564833785408 -> 2564833785312
	2564833785408 [label=ThnnConv2DBackward]
	2564831057952 -> 2564833785408
	2564831057952 [label=_uqBackward]
	2564833785696 -> 2564831057952
	2564833785696 [label=ReluBackward0]
	2564833785840 -> 2564833785696
	2564833785840 [label=NativeBatchNormBackward]
	2564833785888 -> 2564833785840
	2564833785888 [label=ThnnConv2DBackward]
	2564831057504 -> 2564833785888
	2564831057504 [label=_uqBackward]
	2564833786272 -> 2564831057504
	2564833786272 [label=ReluBackward0]
	2564833786416 -> 2564833786272
	2564833786416 [label=NativeBatchNormBackward]
	2564833786464 -> 2564833786416
	2564833786464 [label=ThnnConv2DBackward]
	2564833786752 -> 2564833786464
	2564833786752 [label=AddBackward0]
	2564833786896 -> 2564833786752
	2564828354624 [label="encoder.0.block.0.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	2564828354624 -> 2564833786896
	2564833786896 [label=AccumulateGrad]
	2564833786704 -> 2564833786464
	2564828354752 [label="encoder.0.block.0.bias
 (32)" fillcolor=lightblue]
	2564828354752 -> 2564833786704
	2564833786704 [label=AccumulateGrad]
	2564833786320 -> 2564833786416
	2564828354944 [label="encoder.0.block.1.weight
 (32)" fillcolor=lightblue]
	2564828354944 -> 2564833786320
	2564833786320 [label=AccumulateGrad]
	2564833786560 -> 2564833786416
	2564828355008 [label="encoder.0.block.1.bias
 (32)" fillcolor=lightblue]
	2564828355008 -> 2564833786560
	2564833786560 [label=AccumulateGrad]
	2564833786224 -> 2564831057504
	2564828355904 [label="encoder.0.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564828355904 -> 2564833786224
	2564833786224 [label=AccumulateGrad]
	2564833789312 -> 2564831057504
	2564833789312 [label="
 (1, 32, 14, 14)" fillcolor=orange]
	2564833789824 -> 2564831057504
	2564833789824 [label="
 (1, 32, 14, 14)" fillcolor=orange]
	2564831057280 -> 2564833785888
	2564831057280 [label=_pqBackward]
	2564833786608 -> 2564831057280
	2564833786608 [label=DivBackward0]
	2564833787136 -> 2564833786608
	2564833787136 [label=AddBackward0]
	2564833787040 -> 2564833787136
	2564293358208 [label="encoder.1.block.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2564293358208 -> 2564833787040
	2564833787040 [label=AccumulateGrad]
	2564833786656 -> 2564831057280
	2564828242176 [label="encoder.1.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564828242176 -> 2564833786656
	2564833786656 [label=AccumulateGrad]
	2564833789824 -> 2564831057280
	2564833789824 [label="
 (64, 32, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564831057280
	2564833789312 [label="
 (64, 32, 3, 3)" fillcolor=orange]
	2564833786176 -> 2564833785888
	2564828242496 [label="encoder.1.block.0.bias
 (64)" fillcolor=lightblue]
	2564828242496 -> 2564833786176
	2564833786176 [label=AccumulateGrad]
	2564833785744 -> 2564833785840
	2564828242624 [label="encoder.1.block.1.weight
 (64)" fillcolor=lightblue]
	2564828242624 -> 2564833785744
	2564833785744 [label=AccumulateGrad]
	2564833785984 -> 2564833785840
	2564828225984 [label="encoder.1.block.1.bias
 (64)" fillcolor=lightblue]
	2564828225984 -> 2564833785984
	2564833785984 [label=AccumulateGrad]
	2564833785648 -> 2564831057952
	2564830900480 [label="encoder.1.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830900480 -> 2564833785648
	2564833785648 [label=AccumulateGrad]
	2564833789312 -> 2564831057952
	2564833789312 [label="
 (1, 64, 7, 7)" fillcolor=orange]
	2564833789824 -> 2564831057952
	2564833789824 [label="
 (1, 64, 7, 7)" fillcolor=orange]
	2564831059296 -> 2564833785408
	2564831059296 [label=_pqBackward]
	2564833786032 -> 2564831059296
	2564833786032 [label=DivBackward0]
	2564833787232 -> 2564833786032
	2564833787232 [label=AddBackward0]
	2564833787088 -> 2564833787232
	2564828243328 [label="encoder.2.block.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2564828243328 -> 2564833787088
	2564833787088 [label=AccumulateGrad]
	2564833786128 -> 2564831059296
	2564828242816 [label="encoder.2.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564828242816 -> 2564833786128
	2564833786128 [label=AccumulateGrad]
	2564833789824 -> 2564831059296
	2564833789824 [label="
 (128, 64, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564831059296
	2564833789312 [label="
 (128, 64, 3, 3)" fillcolor=orange]
	2564833785600 -> 2564833785408
	2564828243008 [label="encoder.2.block.0.bias
 (128)" fillcolor=lightblue]
	2564828243008 -> 2564833785600
	2564833785600 [label=AccumulateGrad]
	2564833785360 -> 2564833785312
	2564828243520 [label="encoder.2.block.1.weight
 (128)" fillcolor=lightblue]
	2564828243520 -> 2564833785360
	2564833785360 [label=AccumulateGrad]
	2564833785216 -> 2564833785312
	2564830900928 [label="encoder.2.block.1.bias
 (128)" fillcolor=lightblue]
	2564830900928 -> 2564833785216
	2564833785216 [label=AccumulateGrad]
	2564833785120 -> 2564831059072
	2564830901568 [label="encoder.2.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830901568 -> 2564833785120
	2564833785120 [label=AccumulateGrad]
	2564833789312 -> 2564831059072
	2564833789312 [label="
 (1, 128, 4, 4)" fillcolor=orange]
	2564833789824 -> 2564831059072
	2564833789824 [label="
 (1, 128, 4, 4)" fillcolor=orange]
	2564831058848 -> 2564833784880
	2564831058848 [label=_pqBackward]
	2564833785456 -> 2564831058848
	2564833785456 [label=DivBackward0]
	2564833786368 -> 2564833785456
	2564833786368 [label=AddBackward0]
	2564833786848 -> 2564833786368
	2564828244160 [label="encoder.3.block.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2564828244160 -> 2564833786848
	2564833786848 [label=AccumulateGrad]
	2564833785552 -> 2564831058848
	2564828243392 [label="encoder.3.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564828243392 -> 2564833785552
	2564833785552 [label=AccumulateGrad]
	2564833789824 -> 2564831058848
	2564833789824 [label="
 (256, 128, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564831058848
	2564833789312 [label="
 (256, 128, 3, 3)" fillcolor=orange]
	2564833785072 -> 2564833784880
	2564828244096 [label="encoder.3.block.0.bias
 (256)" fillcolor=lightblue]
	2564828244096 -> 2564833785072
	2564833785072 [label=AccumulateGrad]
	2564833784832 -> 2564833784784
	2564828244416 [label="encoder.3.block.1.weight
 (256)" fillcolor=lightblue]
	2564828244416 -> 2564833784832
	2564833784832 [label=AccumulateGrad]
	2564833784688 -> 2564833784784
	2564830902016 [label="encoder.3.block.1.bias
 (256)" fillcolor=lightblue]
	2564830902016 -> 2564833784688
	2564833784688 [label=AccumulateGrad]
	2564833784592 -> 2564831058624
	2564830902720 [label="encoder.3.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830902720 -> 2564833784592
	2564833784592 [label=AccumulateGrad]
	2564833789312 -> 2564831058624
	2564833789312 [label="
 (1, 256, 2, 2)" fillcolor=orange]
	2564833789824 -> 2564831058624
	2564833789824 [label="
 (1, 256, 2, 2)" fillcolor=orange]
	2564833784016 -> 2564833784304
	2564833784016 [label=TBackward]
	2564833785024 -> 2564833784016
	2564833785024 [label=DivBackward0]
	2564833784736 -> 2564833785024
	2564833784736 [label=RoundBackward]
	2564833785792 -> 2564833784736
	2564833785792 [label=MulBackward0]
	2564833786080 -> 2564833785792
	2564828245888 [label="fc_var.block.weight
 (128, 1024)" fillcolor=lightblue]
	2564828245888 -> 2564833786080
	2564833786080 [label=AccumulateGrad]
	2564833783920 -> 2564833742704
	2564833783920 [label=AddmmBackward]
	2564833784256 -> 2564833783920
	2564830903296 [label="fc_mu.block.bias
 (128)" fillcolor=lightblue]
	2564830903296 -> 2564833784256
	2564833784256 [label=AccumulateGrad]
	2564833784352 -> 2564833783920
	2564833784160 -> 2564833783920
	2564833784160 [label=TBackward]
	2564833784448 -> 2564833784160
	2564833784448 [label=DivBackward0]
	2564833784976 -> 2564833784448
	2564833784976 [label=RoundBackward]
	2564833787184 -> 2564833784976
	2564833787184 [label=MulBackward0]
	2564833787280 -> 2564833787184
	2564828228224 [label="fc_mu.block.weight
 (128, 1024)" fillcolor=lightblue]
	2564828228224 -> 2564833787280
	2564833787280 [label=AccumulateGrad]
	2564833742560 -> 2564833742656
	2564833742560 [label=TBackward]
	2564833784544 -> 2564833742560
	2564833784544 [label=DivBackward0]
	2564833785264 -> 2564833784544
	2564833785264 [label=RoundBackward]
	2564833787376 -> 2564833785264
	2564833787376 [label=MulBackward0]
	2564833784928 -> 2564833787376
	2564828295424 [label="decoder_input.block.weight
 (1024, 128)" fillcolor=lightblue]
	2564828295424 -> 2564833784928
	2564833784928 [label=AccumulateGrad]
	2564831058400 -> 2564833742320
	2564831058400 [label=_pqBackward]
	2564833742800 -> 2564831058400
	2564833742800 [label=DivBackward0]
	2564833787328 -> 2564833742800
	2564833787328 [label=AddBackward0]
	2564833787424 -> 2564833787328
	2564828228352 [label="decoder.0.block.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2564828228352 -> 2564833787424
	2564833787424 [label=AccumulateGrad]
	2564833742608 -> 2564831058400
	2564828244736 [label="decoder.0.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564828244736 -> 2564833742608
	2564833742608 [label=AccumulateGrad]
	2564833789824 -> 2564831058400
	2564833789824 [label="
 (256, 128, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564831058400
	2564833789312 [label="
 (256, 128, 3, 3)" fillcolor=orange]
	2564833742464 -> 2564833742320
	2564828245056 [label="decoder.0.block.0.bias
 (128)" fillcolor=lightblue]
	2564828245056 -> 2564833742464
	2564833742464 [label=AccumulateGrad]
	2564833742272 -> 2564833742224
	2564828244288 [label="decoder.0.block.1.weight
 (128)" fillcolor=lightblue]
	2564828244288 -> 2564833742272
	2564833742272 [label=AccumulateGrad]
	2564833742128 -> 2564833742224
	2564828295808 [label="decoder.0.block.1.bias
 (128)" fillcolor=lightblue]
	2564828295808 -> 2564833742128
	2564833742128 [label=AccumulateGrad]
	2564833742032 -> 2564831059520
	2564830904000 [label="decoder.0.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830904000 -> 2564833742032
	2564833742032 [label=AccumulateGrad]
	2564833789312 -> 2564831059520
	2564833789312 [label="
 (1, 128, 4, 4)" fillcolor=orange]
	2564833789824 -> 2564831059520
	2564833789824 [label="
 (1, 128, 4, 4)" fillcolor=orange]
	2564833688256 -> 2564833741792
	2564833688256 [label=_pqBackward]
	2564833742368 -> 2564833688256
	2564833742368 [label=DivBackward0]
	2564833742176 -> 2564833742368
	2564833742176 [label=AddBackward0]
	2564833784496 -> 2564833742176
	2564828296512 [label="decoder.1.block.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2564828296512 -> 2564833784496
	2564833784496 [label=AccumulateGrad]
	2564833742416 -> 2564833688256
	2564828295936 [label="decoder.1.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564828295936 -> 2564833742416
	2564833742416 [label=AccumulateGrad]
	2564833789824 -> 2564833688256
	2564833789824 [label="
 (128, 64, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564833688256
	2564833789312 [label="
 (128, 64, 3, 3)" fillcolor=orange]
	2564833741984 -> 2564833741792
	2564828296128 [label="decoder.1.block.0.bias
 (64)" fillcolor=lightblue]
	2564828296128 -> 2564833741984
	2564833741984 [label=AccumulateGrad]
	2564833741744 -> 2564833741696
	2564828296704 [label="decoder.1.block.1.weight
 (64)" fillcolor=lightblue]
	2564828296704 -> 2564833741744
	2564833741744 [label=AccumulateGrad]
	2564833741600 -> 2564833741696
	2564830949632 [label="decoder.1.block.1.bias
 (64)" fillcolor=lightblue]
	2564830949632 -> 2564833741600
	2564833741600 [label=AccumulateGrad]
	2564833741504 -> 2564833688480
	2564830950272 [label="decoder.1.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830950272 -> 2564833741504
	2564833741504 [label=AccumulateGrad]
	2564833789312 -> 2564833688480
	2564833789312 [label="
 (1, 64, 8, 8)" fillcolor=orange]
	2564833789824 -> 2564833688480
	2564833789824 [label="
 (1, 64, 8, 8)" fillcolor=orange]
	2564833688704 -> 2564833741264
	2564833688704 [label=_pqBackward]
	2564833741840 -> 2564833688704
	2564833741840 [label=DivBackward0]
	2564833741888 -> 2564833741840
	2564833741888 [label=AddBackward0]
	2564833787520 -> 2564833741888
	2564828297600 [label="decoder.2.block.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2564828297600 -> 2564833787520
	2564833787520 [label=AccumulateGrad]
	2564833741936 -> 2564833688704
	2564828297024 [label="decoder.2.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564828297024 -> 2564833741936
	2564833741936 [label=AccumulateGrad]
	2564833789824 -> 2564833688704
	2564833789824 [label="
 (64, 32, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564833688704
	2564833789312 [label="
 (64, 32, 3, 3)" fillcolor=orange]
	2564833741456 -> 2564833741264
	2564828297536 [label="decoder.2.block.0.bias
 (32)" fillcolor=lightblue]
	2564828297536 -> 2564833741456
	2564833741456 [label=AccumulateGrad]
	2564833741216 -> 2564833741168
	2564828297856 [label="decoder.2.block.1.weight
 (32)" fillcolor=lightblue]
	2564828297856 -> 2564833741216
	2564833741216 [label=AccumulateGrad]
	2564833741072 -> 2564833741168
	2564830950784 [label="decoder.2.block.1.bias
 (32)" fillcolor=lightblue]
	2564830950784 -> 2564833741072
	2564833741072 [label=AccumulateGrad]
	2564833740976 -> 2564833688928
	2564830951488 [label="decoder.2.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830951488 -> 2564833740976
	2564833740976 [label=AccumulateGrad]
	2564833789312 -> 2564833688928
	2564833789312 [label="
 (1, 32, 16, 16)" fillcolor=orange]
	2564833789824 -> 2564833688928
	2564833789824 [label="
 (1, 32, 16, 16)" fillcolor=orange]
	2564833689152 -> 2564833740736
	2564833689152 [label=_pqBackward]
	2564833741312 -> 2564833689152
	2564833741312 [label=DivBackward0]
	2564833741360 -> 2564833741312
	2564833741360 [label=AddBackward0]
	2564833741120 -> 2564833741360
	2564830952512 [label="final_layer.block.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2564830952512 -> 2564833741120
	2564833741120 [label=AccumulateGrad]
	2564833741408 -> 2564833689152
	2564830953088 [label="final_layer.block.0.weight_quant.wgt_alpha
 ()" fillcolor=lightblue]
	2564830953088 -> 2564833741408
	2564833741408 [label=AccumulateGrad]
	2564833789824 -> 2564833689152
	2564833789824 [label="
 (32, 32, 3, 3)" fillcolor=orange]
	2564833789312 -> 2564833689152
	2564833789312 [label="
 (32, 32, 3, 3)" fillcolor=orange]
	2564833740928 -> 2564833740736
	2564830952640 [label="final_layer.block.0.bias
 (32)" fillcolor=lightblue]
	2564830952640 -> 2564833740928
	2564833740928 [label=AccumulateGrad]
	2564833740688 -> 2564833740640
	2564830953216 [label="final_layer.block.1.weight
 (32)" fillcolor=lightblue]
	2564830953216 -> 2564833740688
	2564833740688 [label=AccumulateGrad]
	2564833740544 -> 2564833740640
	2564830953408 [label="final_layer.block.1.bias
 (32)" fillcolor=lightblue]
	2564830953408 -> 2564833740544
	2564833740544 [label=AccumulateGrad]
	2564833740448 -> 2564833689376
	2564830991104 [label="final_layer.block.2.act_alpha
 ()" fillcolor=lightblue]
	2564830991104 -> 2564833740448
	2564833740448 [label=AccumulateGrad]
	2564833789312 -> 2564833689376
	2564833789312 [label="
 (1, 32, 32, 32)" fillcolor=orange]
	2564833789824 -> 2564833689376
	2564833789824 [label="
 (1, 32, 32, 32)" fillcolor=orange]
	2564833740352 -> 2564833739920
	2564833740352 [label=AddBackward0]
	2564833740592 -> 2564833740352
	2564830991296 [label="final_layer.block.3.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	2564830991296 -> 2564833740592
	2564833740592 [label=AccumulateGrad]
	2564833740064 -> 2564833739920
	2564830991424 [label="final_layer.block.3.bias
 (1)" fillcolor=lightblue]
	2564830991424 -> 2564833740064
	2564833740064 [label=AccumulateGrad]
	2564833740160 -> 2564833759296
	2564830991808 [label="final_layer.block.4.act_alpha
 ()" fillcolor=lightblue]
	2564830991808 -> 2564833740160
	2564833740160 [label=AccumulateGrad]
	2564833789824 -> 2564833759296
	2564833789824 [label="
 (1, 1, 32, 32)" fillcolor=orange]
	2564833790080 -> 2564833759296
	2564833790080 [label="
 (1, 1, 32, 32)" fillcolor=orange]
	2564833759296 -> 2564833755456
	2564833705088 [label="
 (1, 128)" fillcolor=darkolivegreen1]
	2564833783920 -> 2564833705088
	2564833705664 [label="
 (1, 128)" fillcolor=darkolivegreen1]
	2564833784304 -> 2564833705664
}
